В статистике уравнение остатков обычно используется для вычисления различий между наблюдаемыми значениями зависимой переменной и прогнозируемыми значениями этой переменной на основе регрессионной модели.

В матричном виде уравнение остатков выражается как:

E = F - GC

где:

E - вектор остатков n x 1 (т.е. различий между наблюдаемыми и прогнозируемыми значениями)
F - это вектор n x 1 наблюдаемых значений зависимой переменной
G - это матрица n x k переменных-предикторов (также называемых независимыми или объясняющими переменными) в регрессионной модели
C - это вектор коэффициентов k x 1 (также называемых параметрами регрессии или весами), оцененный регрессионной моделью
Матричное умножение G и C дает вектор k x 1 прогнозируемых значений зависимой переменной, который при вычитании из F дает вектор остатков E.

Другими словами, уравнение остатков в матричном виде представляет разницу между наблюдаемыми значениями зависимой переменной и прогнозируемыми значениями этой переменной на основе набора переменных-предикторов и их соответствующих коэффициентов регрессии.



I(p)=sum(y-Pn(x))^2=E^T*E

Формула, на которую вы ссылаетесь, является формулой метода наименьших квадратов для подгонки многочлена степени n к набору точек данных (x, y).

Давайте разберем формулу:

Сначала мы определяем многочлен степени n как Pn(x) = a0 + a1x + a2x ^ 2 + ... + an * x ^ n. Этот многочлен имеет n + 1 коэффициентов, которые нам нужно будет определить.

Мы хотим найти значения этих коэффициентов, которые минимизируют сумму квадратов ошибок между прогнозируемыми значениями Pn(x) и фактическими значениями y в каждой точке x. Итак, мы определяем термин ошибки Ei = y_i - Pn(x_i) для каждой точки данных (x_i, y_i).

Затем мы можем определить сумму квадратов ошибок как I(p) = sum(Ei ^ 2) = sum(y_i - Pn(x_i)) ^ 2.

Мы хотим найти значения коэффициентов a0, a1, ..., an, которые минимизируют I(p). Для этого мы берем производную от I(p) по каждому коэффициенту и устанавливаем их равными 0:

dI(p)/da0 = -2sum(y_i - a0 - a1x_i - ... - anx_i^n) = 0
dI(p)/da1 = -2sum(x_i*(y_i - a0 - a1x_i - ... - anx_i^n)) = 0
...
dI(p)/dan = -2sum(x_i^n(y_i - a0 - a1x_i - ... - anx_i^n)) = 0

Это дает нам систему из n + 1 уравнений с n + 1 неизвестными (коэффициентами). Мы можем записать это в матричном виде как Gp = b, где G - матрица n + 1 на n + 1, p - вектор коэффициентов, а b - вектор длины n + 1 с элементами b0 = sum(y_i), b1 = sum(x_iy_i), ..., bn = сумма(x_i^n*y_i).

Мы можем решить эту систему уравнений для p, используя матричную алгебру: p = (G ^ T * G) ^ (-1) * G ^ T * b.

Как только у нас будет p, мы можем включить эти коэффициенты в наш многочлен Pn (x), чтобы получить полином, наиболее подходящий для набора данных.

Итак, вкратце, формула I(p) = sum(y_i - Pn(x_i)) ^ 2 представляет собой сумму квадратов ошибок между прогнозируемыми значениями полинома Pn(x) и фактическими значениями y в каждой точке x. Цель метода наименьших квадратов - найти значения коэффициентов, которые минимизируют эту сумму квадратов ошибок.